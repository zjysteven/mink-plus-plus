<!DOCTYPE html>
<html>

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-DERZX1PWZ4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-DERZX1PWZ4');
  </script>

  <meta name="google-site-verification" content="Bi88m3QClAiFVPoDVcFRkhgAEYFU4M1M4khtU37bUlg" />
  <meta charset="utf-8">
  <meta name="description" content="Min-K%++">
  <meta name="keywords"
    content="Membership Inference Attack, Copyrighted Information Detection, Dataset Contamination, Data Extraction, LLM, LLaMA, Mamba, GPT, WikiMIA, Min-K, min-k">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Min-K%++</title>

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/tifa.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->


  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yushi-hu.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
            Fine-Grained RLHF
          </a>
          <a class="navbar-item" href="https://yushi-hu.github.io/promptcap_demo/">
            PromptCap
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <!--<span class="big-emoji">&#x1F575;&#xFE0F;</span>-->
              Min-K%++: Improved Baseline for Detecting Pre-Training Data of Large Language Models
            </h1>
            <div class="is-size-5">
              <span class="author-block">
                <a href="http://zjysteven.github.io">Jingyang Zhang</a><sup>1</sup><sup>*</sup>,
              </span>
              <span class="author-block">
                <a href="https://jingwei-sun.com/">Jingwei Sun</a><sup>1</sup><sup>*</sup>,
              </span>
              <span class="author-block">
                <a href="https://eric-yeats.com/">Eric Yeats</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://oyy2000.github.io">Yang Ouyang</a><sup>1</sup>,
              </span><br>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/martinkuo0427/">Martin Kuo</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://cei.pratt.duke.edu/people/jianyi-zhang">Jianyi Zhang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.haofrankyang.net/">Hao Yang</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://cei.pratt.duke.edu/people/hai-helen-li">Hai Li</a><sup>1</sup>
              </span>
            </div>
  
            <br>
            <div class="is-size-5 publication-authors">
             <span class="author-block">&nbsp;&nbsp;<sup>*</sup>Equal Contribution</span><br>
              <span class="author-block"><sup>1</sup>Duke University</span>
              <span class="author-block"><sup>2</sup>Johns Hopkins University</span>
            </div>

            
              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="https://zjysteven.github.io/mink-plus-plus/" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper (coming soon)</span>
                    </a>
                  </span>

              <!-- code Link. -->
              <span class="link-block">
                <a href="https://github.com/zjysteven/mink-plus-plus" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

                <!-- Dataset Link. -->
                <!--<span class="link-block">
                  <a href="https://huggingface.co/datasets/swj0419/WikiMIA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>WikiMIA datasets</span>
                  </a>
                </span>-->

                  <!-- Dataset Link. -->
                <!--<span class="link-block">
                  <a href="https://huggingface.co/datasets/swj0419/BookMIA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>BookMIA datasets</span>
                  </a>
                </span>-->
    
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
  <img src="./static/images/teaser_w_results.png" alt="teaser">

      


<h3 class="subtitle">
    <style>
      .subtitle a {
        color: blue;
      }
    </style>
  We propose a novel method for the detection of pre-training data of LLMs. This problem (see above figure left panel
  for an illustration) has been receiving growing attention recently, due to its profound implications to copyrighted content detection,
  privacy auditing, and evaluation data contamination.<br><br> 
  
  Our method, named <b>Min-K%++</b>, is theoretically motivated by revisiting LLM's 
  training objective (maximum likelihood estimation) through the lens of score matching. We show that LLM training implicitly minimizes
  the Hessian trace of log likelihood, which encodes rich second-order information and can thus serve as a robust indicator for flagging training data.<br><br>

  Empirically, Min-K%++ achieves state-of-the-art performance on the <a href="#wikimia">WikiMIA</a> benchmark, outperforming existing approaches by large margin (showcased by the above figure right panel). 
  On the more challenging <a href="#mimir">MIMIR</a> benchmark, Min-K%++ is also the best among reference-free methods and performs on par with reference-based methods.

</h3>


    </div>
  </div>
</section>

<!--
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed.
            Given the
            incredible scale of this data, up to trillions of tokens, it's nearly certain it includes potentially
            problematic
            text such as copyrighted materials, personally identifiable information, and test data for widely reported
            reference
            benchmarks. However, we currently lack knowledge on which data of these types is included or in what
            proportions.
          </p>

           <li><b>Pretraining Ddata detection problem</b>. In this paper, we explore the pretraining data detection problem<span>&#x1F575;&#xFE0F;</span>: given a piece of
            text and
            black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained
            on the
            provided text? 
            
            <li><b>Dynamic benchmark WikiMIA</b>. To aid this study, we present <b><a href="https://huggingface.co/datasets/swj0419/WikiMIA"
                target="_blank">
                a dynamic benchmark WikiMIA</a></b> <span>&#128214;</span> that uses data created both before and
            after model training
            to support gold truth detection.

          <li><b>Detection method Min-K% Prob</b>. We also design a new <b><a href="https://github.com/swj0419/detect-pretrain-code" target="_blank">detection
                method Min-K%
                Prob</a></b> <img src="static/images/min-k_icon.png" alt="Min-K Icon" style="width:25px; height:25px;">. This is built
            on a straightforward
            hypothesis: an unobserved example is more likely to have a few outlier words with low probabilities under
            the LLM,
            while a recognized example is less inclined to contain words with such reduced probabilities. Min-K% Prob
            operates
            without any insight into the pretraining corpus or any extra training, distinguishing it from past detection
            strategies
            that necessitate educating a reference model on data analogous to the pretraining data. Furthermore, our
            tests indicate
            that Min-K% Prob delivers a 7.4% enhancement on WikiMIA relative to these preceding techniques. 

            <li><b>Real-life use cases</b>. We employ
            Min-K% Prob in
            three real-life contexts: <strong>benchmark example contamination detection</strong>,
            <a href="#privacy-auditing"><strong>privacy auditing of machine unlearning</strong></a>,
            and <a href="#copyrighted-detection"><strong>copyrighted text detection</strong></a> in language models' pretraining
            data.
        </div>
      </div>
    </div>
  </div>
</section>
-->


<!--
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Detection Method Min-K% Prob</h2>
        <div class="content has-text-justified">
          <p>
            <strong>What is Min-K% Prob?</strong><br>
            We propose a pretraining data detection method named Min-K% Prob. Our method is based on a simple
            hypothesis: an unseen example tends to contain a few outlier words with low probabilities, whereas a seen
            example is less likely to contain words with such low probabilities. MIN-K% Prob computes the average
            probabilities of outlier tokens.
          </p>

          <p>
            <strong>How to use Min-K% Prob?</strong><br>
            To check if a text was in LLM's pretraining:
          <ol>
            <li>Evaluate token probabilities in the text.</li>
            <li>Pick the k% tokens with minimum probabilities.</li>
            <li>Compute their average log likelihood.</li>
          </ol>
          If the average log likelihood is high, the text is likely in the pretraining data. ✅
          </p>

    <a href="https://arxiv.org/pdf/2310.16789.pdf" target="_blank">See more results in our paper</a>
        </div>
      </div>
    </div>
  </div>
</section>
-->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3" id="copyrighted-detection">Copyrighted Book Detection</h2> -->
        <h2 class="title is-3" id="wikimia"><a href="https://github.com/swj0419/detect-pretrain-code/">WikiMIA</a> results</h2>
        <div class="content has-text-justified">

        <style>
          table {
            width: 100%;
            border-collapse: collapse;
          }

          table,
          th,
          td {
            border: 1px solid black;
          }

          th,
          td {
            padding: 8px;
            text-align: left;
          }

          th {
            background-color: #f2f2f2;
          }
        </style>
      </head>

      <body>

        <table>
          <caption>Detection AUROC (%) on WikiMIA_length32. <b>Min-K%++ achieves significantly improved results over Min-K% and other existing methods.</b>
            For more results, don't hesitate to check our paper.
          </caption>
          <thead>
            <tr>
              <th>Method</th>
              <th>Mamba-1.4B</th>
              <th>Pythia-6.9B</th>
              <th>LLaMA-13B</th>
              <th>LLaMA-30B</th>
              <th>LLaMA-65B</th>
              <th>Average</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Loss</td>
              <td>61.0</td>
              <td>63.8</td>
              <td>67.5</td>
              <td>69.4</td>
              <td>70.7</td>
              <td>66.5</td>
            </tr>

            <tr>
                <td>Ref</td>
                <td>62.2</td>
                <td>63.6</td>
                <td>57.9</td>
                <td>63.5</td>
                <td>68.8</td>
                <td>63.2</td>
            </tr>

            <tr>
                <td>Lowercase</td>
                <td>60.9</td>
                <td>62.2</td>
                <td>64.0</td>
                <td>64.1</td>
                <td>66.5</td>
                <td>63.5</td>
              </tr>

            <tr>
                <td>Zlib</td>
                <td>61.9</td>
                <td>64.3</td>
                <td>67.8</td>
                <td>69.8</td>
                <td>71.1</td>
                <td>67.0</td>
              </tr>

            <tr>
                <td>Neighbor</td>
                <td>64.1</td>
                <td>65.8</td>
                <td>65.8</td>
                <td>67.6</td>
                <td>69.6</td>
                <td>66.6</td>
              </tr>

            <tr>
                <td>Min-K%</td>
                <td>63.2</td>
                <td>66.3</td>
                <td>68.0</td>
                <td>70.1</td>
                <td>71.3</td>
                <td>67.8</td>
              </tr>

            <tr>
                <td>Min-K%++</td>
                <td><b>66.8</b></td>
                <td><b>70.3</b></td>
                <td><b>84.8</b></td>
                <td><b>84.3</b></td>
                <td><b>85.1</b></td>
                <td><b>78.3</b></td>
              </tr>
            
          </tbody>
        </table>

      </body>

      </html>

    </div>
  </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3" id="mimir"><a href="https://github.com/iamgroot42/mimir">MIMIR</a> results</h2>
          <div class="content has-text-justified">
            
              <style>
                  table {
                    width: 100%;
                    border-collapse: collapse;
                  }
        
                  table,
                  th,
                  td {
                    border: 1px solid black;
                  }
        
                  th,
                  td {
                    padding: 8px;
                    text-align: left;
                  }
        
                  th {
                    background-color: #f2f2f2;
                  }
                </style>
              </head>
        
              <body>
        
                <table>
                  <caption>
                      Detection AUROC (%) on MIMIR averaged over 7 subdomains.
                      The best result is <b>bolded</b>, with the runner-up <u>underlined</u>.
                      <b>Min-K%++ achieves SOTA among reference-free methods and performs on par with the Ref method which requires an extra reference LLM.</b>
                  </caption>
                  <thead>
                    <tr>
                      <th>Method</th>
                      <th>Pythia-160M</th>
                      <th>Pythia-1.4B</th>
                      <th>Pythia-2.8B</th>
                      <th>Pythia-6.9B</th>
                      <th>Pythia-12B</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Loss</td>
                      <td>52.1</td>
                      <td>53.1</td>
                      <td>53.5</td>
                      <td>54.4</td>
                      <td>54.9</td>
                    </tr>
        
                    <tr>
                        <td>Ref</td>
                        <td>52.2</td>
                        <td><b>54.6</b></td>
                        <td><b>55.6</b></td>
                        <td><b>57.4</b></td>
                        <td><u>58.7</u></td>
                    </tr>
        
                    <tr>
                        <td>Zlib</td>
                        <td>52.3</td>
                        <td>53.2</td>
                        <td>53.6</td>
                        <td>54.3</td>
                        <td>54.8</td>
                      </tr>
        
                    <tr>
                        <td>Neighbor</td>
                        <td>52.0</td>
                        <td>52.9</td>
                        <td>53.2</td>
                        <td>53.8</td>
                        <td>/</td>
                      </tr>
        
                    <tr>
                        <td>Min-K%</td>
                        <td><b>52.6</b></td>
                        <td>53.6</td>
                        <td>54.2</td>
                        <td>55.2</td>
                        <td>55.9</td>
                      </tr>
        
                    <tr>
                        <td>Min-K%++</td>
                        <td><u>52.4</u></td>
                        <td><u>54.1</u></td>
                        <td><u>55.3</u></td>
                        <td><u>57.0</u></td>
                        <td><b>58.7</b></td>
                      </tr>
                    
                  </tbody>
                </table>
        
  
          </div>
        </div>
      </div>
    </div>
  </section>


<section class="section is-light" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>
Coming soon...
</code></pre>
        </div>
      </section>



      <footer class="footer">
        <div class="container">
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p>
                  This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>. This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                    code</a> of this website, we just ask that you link back to this page in the footer.
                </p>
              </div>
            </div>
          </div>
        </div>
      </footer>

</body>

</html>
